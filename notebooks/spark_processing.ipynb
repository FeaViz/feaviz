{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install apache-superset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install cassandra-driver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cassandra\n",
    "# print(cassandra.*version*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_file_path = \"/Users/lavanyamk/Documents/FeaViz/Olist_ecommerce_data/olist_products_dataset_output.csv\"\n",
    "db_bundle_path = \"/Users/lavanyamk/Downloads/secure-connect-nosql-db.zip\"\n",
    "user_name = \"CgYxKnkrerLwiDCMYlNXdPHa\"\n",
    "password = \"91YcRxuTwnZR-.WB-TJa5HOoFL8oL,AdmQusxObsSK5pIriJM0kFX9oSgr1jj3BakDZ5jOz4AZy0rpOwT76JWUNQWTC8u1an5FmJs-FR1xsczTLxJXFGUZczuGCMJ9aK\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %env PYSPARK_SUBMIT_ARGS=' --packages com.datastax.spark:spark-cassandra-connector_2.12:3.0.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cassandra.cluster import Cluster\n",
    "from cassandra.auth import PlainTextAuthProvider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, SQLContext\n",
    "from pyspark.sql.functions import col, from_unixtime\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName(\"temp\").getOrCreate()\n",
    "\n",
    "\n",
    "# spark = SparkSession \\\n",
    "#     .builder \\\n",
    "#     .appName('demo') \\\n",
    "#     .master(\"local\") \\\n",
    "#     .config( \\\n",
    "#         \"spark.cassandra.connection.config.cloud.path\", \\\n",
    "#         \"file:\"+db_bundle_path) \\\n",
    "#     .config(\"spark.cassandra.auth.username\", user_name) \\\n",
    "#     .config(\"spark.cassandra.auth.password\", password) \\\n",
    "#     .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/spark-3.0.1-bin-hadoop3.2\r\n"
     ]
    }
   ],
   "source": [
    "# spark.sparkContext.version\n",
    "!echo $SPARK_HOME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.0.1'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sparkContext.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"org.apache.spark.sql.cassandra\"\n",
    "                            ).options(table=\"users_by_city\", keyspace=\"nosql1\").load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Upload csv to AstraDB\n",
    "# TODO: Read from AstraDB\n",
    "features_df = spark.read.option(\"inferSchema\", \"true\").csv(\n",
    "    \"file://\"+csv_file_path, \n",
    "    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import Row\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transpose_df(df, columns, pivotCol):\n",
    "    columnsValue = list(map(lambda x: str(\"'\") + str(x) + str(\"',\")  + str(x), columns))\n",
    "    stackCols = ','.join(x for x in columnsValue)\n",
    "    df_1 = df.selectExpr(pivotCol, \"stack(\" + str(len(columns)) + \",\" + stackCols + \")\")\\\n",
    "           .select(pivotCol, \"col0\", \"col1\")\n",
    "    final_df = df_1.groupBy(col(\"col0\")).pivot(pivotCol).agg(concat_ws(\"\", collect_list(col(\"col1\"))))\\\n",
    "                 .withColumnRenamed(\"col0\", pivotCol)\n",
    "    return final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_id</th>\n",
       "      <th>product_category_name</th>\n",
       "      <th>product_name_lenght</th>\n",
       "      <th>product_description_lenght</th>\n",
       "      <th>product_photos_qty</th>\n",
       "      <th>product_weight_g</th>\n",
       "      <th>product_length_cm</th>\n",
       "      <th>product_height_cm</th>\n",
       "      <th>product_width_cm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1e9e8ef04dbcff4541ed26657ea517e5</td>\n",
       "      <td>perfumery</td>\n",
       "      <td>40.0</td>\n",
       "      <td>287.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>225.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>14.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3aa071139cb16b67ca9e5dea641aaa2f</td>\n",
       "      <td>art</td>\n",
       "      <td>44.0</td>\n",
       "      <td>276.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>96bd76ec8810374ed1b65e291975717f</td>\n",
       "      <td>sports_leisure</td>\n",
       "      <td>46.0</td>\n",
       "      <td>250.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>154.0</td>\n",
       "      <td>18.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>cef67bcfe19066a932b7673e239eb23d</td>\n",
       "      <td>baby</td>\n",
       "      <td>27.0</td>\n",
       "      <td>261.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>371.0</td>\n",
       "      <td>26.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>26.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9dc1a7de274444849c219cff195d0b71</td>\n",
       "      <td>housewares</td>\n",
       "      <td>37.0</td>\n",
       "      <td>402.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>625.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>17.0</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32946</th>\n",
       "      <td>a0b7d5a992ccda646f2d34e418fff5a0</td>\n",
       "      <td>furniture_decor</td>\n",
       "      <td>45.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>12300.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32947</th>\n",
       "      <td>bf4538d88321d0fd4412a93c974510e6</td>\n",
       "      <td>construction_tools_lights</td>\n",
       "      <td>41.0</td>\n",
       "      <td>971.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>16.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32948</th>\n",
       "      <td>9a7c6041fa9592d9d9ef6cfe62a71f8c</td>\n",
       "      <td>bed_bath_table</td>\n",
       "      <td>50.0</td>\n",
       "      <td>799.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1400.0</td>\n",
       "      <td>27.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>27.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32949</th>\n",
       "      <td>83808703fc0706a22e264b9d75f04a2e</td>\n",
       "      <td>computers_accessories</td>\n",
       "      <td>60.0</td>\n",
       "      <td>156.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>700.0</td>\n",
       "      <td>31.0</td>\n",
       "      <td>13.0</td>\n",
       "      <td>20.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32950</th>\n",
       "      <td>106392145fca363410d287a815be6de4</td>\n",
       "      <td>bed_bath_table</td>\n",
       "      <td>58.0</td>\n",
       "      <td>309.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2083.0</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32951 rows √ó 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             product_id      product_category_name  \\\n",
       "0      1e9e8ef04dbcff4541ed26657ea517e5                  perfumery   \n",
       "1      3aa071139cb16b67ca9e5dea641aaa2f                        art   \n",
       "2      96bd76ec8810374ed1b65e291975717f             sports_leisure   \n",
       "3      cef67bcfe19066a932b7673e239eb23d                       baby   \n",
       "4      9dc1a7de274444849c219cff195d0b71                 housewares   \n",
       "...                                 ...                        ...   \n",
       "32946  a0b7d5a992ccda646f2d34e418fff5a0            furniture_decor   \n",
       "32947  bf4538d88321d0fd4412a93c974510e6  construction_tools_lights   \n",
       "32948  9a7c6041fa9592d9d9ef6cfe62a71f8c             bed_bath_table   \n",
       "32949  83808703fc0706a22e264b9d75f04a2e      computers_accessories   \n",
       "32950  106392145fca363410d287a815be6de4             bed_bath_table   \n",
       "\n",
       "       product_name_lenght  product_description_lenght  product_photos_qty  \\\n",
       "0                     40.0                       287.0                 1.0   \n",
       "1                     44.0                       276.0                 1.0   \n",
       "2                     46.0                       250.0                 1.0   \n",
       "3                     27.0                       261.0                 1.0   \n",
       "4                     37.0                       402.0                 4.0   \n",
       "...                    ...                         ...                 ...   \n",
       "32946                 45.0                        67.0                 2.0   \n",
       "32947                 41.0                       971.0                 1.0   \n",
       "32948                 50.0                       799.0                 1.0   \n",
       "32949                 60.0                       156.0                 2.0   \n",
       "32950                 58.0                       309.0                 1.0   \n",
       "\n",
       "       product_weight_g  product_length_cm  product_height_cm  \\\n",
       "0                 225.0               16.0               10.0   \n",
       "1                1000.0               30.0               18.0   \n",
       "2                 154.0               18.0                9.0   \n",
       "3                 371.0               26.0                4.0   \n",
       "4                 625.0               20.0               17.0   \n",
       "...                 ...                ...                ...   \n",
       "32946           12300.0               40.0               40.0   \n",
       "32947            1700.0               16.0               19.0   \n",
       "32948            1400.0               27.0                7.0   \n",
       "32949             700.0               31.0               13.0   \n",
       "32950            2083.0               12.0                2.0   \n",
       "\n",
       "       product_width_cm  \n",
       "0                  14.0  \n",
       "1                  20.0  \n",
       "2                  15.0  \n",
       "3                  26.0  \n",
       "4                  13.0  \n",
       "...                 ...  \n",
       "32946              40.0  \n",
       "32947              16.0  \n",
       "32948              27.0  \n",
       "32949              20.0  \n",
       "32950               7.0  \n",
       "\n",
       "[32951 rows x 9 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[product_id: string, product_category_name: string, product_name_lenght: double, product_description_lenght: double, product_photos_qty: double, product_weight_g: double, product_length_cm: double, product_height_cm: double, product_width_cm: double]>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuple([str(m[1]) for m in features_df.dtypes]), features_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ref: https://www.datasciencemadesimple.com/count-of-missing-nanna-and-null-values-in-pyspark/\n",
    "# spark.createDataFrame(features_df.dtypes).show()\n",
    "\n",
    "# spark.createDataFrame([(\"data_type\")] + [(m[1]) for m in features_df.dtypes], [\"summary\"] + features_df.columns)\n",
    "# rdd.toDF(columns).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_summary(features_df):\n",
    "    distict_count_summary_col = lit(\"distinct_count\").alias(\"summary\")\n",
    "    missing_count_summary_col = lit(\"missing_count\").alias(\"summary\")\n",
    "    exprs = [distict_count_summary_col] + [countDistinct(x).alias(x) for x in features_df.columns]\n",
    "\n",
    "    data ={'summary':'data_type'}\n",
    "    data.update( {m[0]:m[1] for m in features_df.dtypes})\n",
    "\n",
    "\n",
    "    distinct_count_df = features_df.agg(*exprs)\n",
    "    missing_df = features_df.select([missing_count_summary_col] +\n",
    "                       [count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in features_df.columns])\n",
    "    dtype_df = spark.createDataFrame([data])\n",
    "\n",
    "    custom_summary_df = dtype_df.unionByName(missing_df).unionByName(distinct_count_df)\n",
    "\n",
    "    summary_t_df = features_df.describe().unionByName(features_df.summary(\"25%\", \"50%\", \"75%\")).unionByName(custom_summary_df)\n",
    "    summary_df = get_transpose_df(summary_t_df, features_df.columns, \"summary\").withColumnRenamed(\"25%\", \"top_25\"\n",
    "                                ).withColumnRenamed(\"50%\", \"top_50\"\n",
    "                                                   ).withColumnRenamed(\"75%\", \"top_75\")\n",
    "    return summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "|             summary|top_25|top_50|top_75|count|data_type|distinct_count|                 max|              mean|                 min|missing_count|            stddev|\n",
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "|  product_photos_qty|   1.0|   1.0|   3.0|32341|   double|            19|                20.0|2.1889861166939797|                 1.0|          610|1.7367656379315435|\n",
      "|product_category_...|      |      |      |32341|   string|            73|       watches_gifts|                  |agro_industry_and...|          610|                  |\n",
      "|    product_weight_g| 300.0| 700.0|1900.0|32949|   double|          2204|             40425.0|2276.4724877841513|                 0.0|            2| 4282.038730977024|\n",
      "|    product_width_cm|  15.0|  20.0|  30.0|32949|   double|            95|               118.0|23.196728277034204|                 6.0|            2|12.079047453227794|\n",
      "| product_name_lenght|  42.0|  51.0|  57.0|32341|   double|            66|                76.0| 48.47694876472589|                 5.0|          610|10.245740725237287|\n",
      "|   product_height_cm|   8.0|  13.0|  21.0|32949|   double|           102|               105.0|16.937661234028347|                 2.0|            2|13.637554061749569|\n",
      "|   product_length_cm|  18.0|  25.0|  38.0|32949|   double|            99|               105.0| 30.81507784758263|                 7.0|            2|16.914458054065953|\n",
      "|product_descripti...| 339.0| 595.0| 972.0|32341|   double|          2960|              3992.0| 771.4952846232337|                 4.0|          610| 635.1152246349538|\n",
      "|          product_id|      |      |      |32951|   string|         32951|fffe9eeff12fcbd74...|                  |00066f42aeeb9f300...|            0|                  |\n",
      "+--------------------+------+------+------+-----+---------+--------------+--------------------+------------------+--------------------+-------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary_df = create_feature_summary(features_df)\n",
    "summary_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to AstraDB with the same DDL of the above table but table is input_file_name_feature_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.coalesce(1).write.format('com.databricks.spark.csv').option(\n",
    "    \"header\",True) .mode('overwrite').csv(\"file:///Users/lavanyamk/Documents/FeaViz/outputs/feature_summary.csv\")\n",
    "# Write to AstraDB with table nam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PARIS\n"
     ]
    }
   ],
   "source": [
    "\n",
    "cloud_config= {\n",
    "        'secure_connect_bundle': db_bundle_path\n",
    "}\n",
    "auth_provider = PlainTextAuthProvider(user_name, password)\n",
    "cluster = Cluster(cloud=cloud_config, auth_provider=auth_provider)\n",
    "session = cluster.connect()\n",
    "\n",
    "row = session.execute(\"select * from nosql1.users_by_city\").one()\n",
    "if row:\n",
    "    print(row[0])\n",
    "else:\n",
    "    print(\"An error occurred.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('summary', 'string'), ('product_id', 'string'), ('product_category_name', 'string'), ('product_name_lenght', 'string'), ('product_description_lenght', 'string'), ('product_photos_qty', 'string'), ('product_weight_g', 'string'), ('product_length_cm', 'string'), ('product_height_cm', 'string'), ('product_width_cm', 'string')]\n"
     ]
    }
   ],
   "source": [
    "print([col for col in summary_df.dtypes])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now both input data & summary data is available AstraDB\n",
    "# Create empty dashboard\n",
    "\n",
    "# Create Feature Summary in Table View\n",
    "    # VISUALIZATION TYPE -> Table\n",
    "    # QUERY MODE -> RAW RECORDS\n",
    "    # COLUMNS -> Check for all columns options\n",
    "# Add this table with full width to dashboard\n",
    "\n",
    "# TODO: Read this dtypes & call superset API to create individual column viz\n",
    "# for i in dtypes:\n",
    "    # Create charts\n",
    "    # Numerical (Double, Int) -> VISUALIZATION TYPE Histogram, COLUMNS is the col_name  ex: product_height_cm_count\n",
    "    # String -> VISUALIZATION TYPE Histogram, COLUMNS is the col_name  ex: product_height_cm_count\n",
    "        # VISUALIZATION TYPE -> Bar Chart\n",
    "        # METRICS -> COUNT(*)\n",
    "        # Series -> col_name\n",
    "\n",
    "# Add all charts to prev dashboard. Each chart has width 1/3.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.execute(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imporpyspark.sql.catalogca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Couldn't find nosql1.feature_summary or any similarly named keyspace and table pairs;",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-5e8d62ceb0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m summary_df.write.format(\"org.apache.spark.sql.cassandra\"\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'append'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                               ).options(table=\"feature_summary\", keyspace=\"nosql1\").save()\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, path, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m    823\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 825\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    826\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jwrite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    132\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m                 \u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark-3.0.1-bin-hadoop3.2/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(e)\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: Couldn't find nosql1.feature_summary or any similarly named keyspace and table pairs;"
     ]
    }
   ],
   "source": [
    "summary_df.write.format(\"org.apache.spark.sql.cassandra\"\n",
    "                        ).mode('append'\n",
    "                              ).options(table=\"feature_summary\", keyspace=\"nosql1\").save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('product_id', 'string'),\n",
       " ('product_category_name', 'string'),\n",
       " ('product_name_lenght', 'double'),\n",
       " ('product_description_lenght', 'double'),\n",
       " ('product_photos_qty', 'double'),\n",
       " ('product_weight_g', 'double'),\n",
       " ('product_length_cm', 'double'),\n",
       " ('product_height_cm', 'double'),\n",
       " ('product_width_cm', 'double')]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|       summary|         review_id|            order_id|        review_score|review_comment_title|review_comment_message|review_creation_date|review_answer_timestamp|\n",
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "|         count|            105188|              102859|              102692|               12176|                 41868|               96025|                  96002|\n",
      "|          mean|               4.5|                 0.0|   4.071667849964501|3.155449396525236...|  1.111111111111111...|                null|                   null|\n",
      "|        stddev|0.7071067811865476|                 0.0|   1.386648877434681|5.616554832455847E11|              Infinity|                null|                   null|\n",
      "|           min|                 \"|                    |                   \"|                    |                      | FOI A MINHA PRIM...|    POIS N√ÉO CUMPREM...|\n",
      "|           max|     ü§ôüèºüëèüèºüëèüèº\"|visando sempre o ...|seria mais coeren...|                 üîü |  üò°üò°üò°üò°üò°üëéüëéüëéüëéüëé|veio bem embalada...|              70 + R$15|\n",
      "|           25%|               4.0|                 0.0|                 4.0|                 5.0|                   8.0|                null|                   null|\n",
      "|           50%|               4.0|                 0.0|                 5.0|                10.0|                  10.0|                null|                   null|\n",
      "|           75%|               5.0|                 0.0|                 5.0|                10.0|                  10.0|                null|                   null|\n",
      "|     data_type|            string|              string|              string|              string|                string|              string|                 string|\n",
      "| missing_count|                 1|                2330|                2497|               93013|                 63321|                9164|                   9187|\n",
      "|distinct_count|            103956|              100562|                2567|                5044|                 36663|                 725|                  95072|\n",
      "+--------------+------------------+--------------------+--------------------+--------------------+----------------------+--------------------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|total_count|\n",
      "+-----------+\n",
      "|      32951|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "features_df.agg(countDistinct(\"product_id\").alias(\"total_count\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "|             summary|count|                 max|              mean|                 min|            stddev|\n",
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "|  product_photos_qty|32341|                   9|2.1889861166939797|                   1|1.7367656379315435|\n",
      "|product_category_...|32341|utilidades_domest...|                  |agro_industria_e_...|                  |\n",
      "|    product_weight_g|32949|                 998|2276.4724877841513|                   0| 4282.038730977024|\n",
      "|    product_width_cm|32949|                  98|23.196728277034204|                  10|12.079047453227794|\n",
      "| product_name_lenght|32341|                   9| 48.47694876472589|                  10|10.245740725237287|\n",
      "|   product_height_cm|32949|                  99|16.937661234028347|                  10|13.637554061749569|\n",
      "|   product_length_cm|32949|                  99| 30.81507784758263|                  10|16.914458054065953|\n",
      "|product_descripti...|32341|                 999| 771.4952846232337|                 100| 635.1152246349538|\n",
      "|          product_id|32951|fffe9eeff12fcbd74...|                  |00066f42aeeb9f300...|                  |\n",
      "+--------------------+-----+--------------------+------------------+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "describe_features_df = get_transpose_df(features_df.describe(), features_df.columns, \"summary\")\n",
    "describe_features_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "@udf(returnType=StringType())\n",
    "def get_distinct_count(col_name_str, features_df):\n",
    "    distinct_count = features_df.select(col_name_str).distinct().count()\n",
    "    return distinct_count \n",
    "\n",
    "# get_distinct_count_udf = udf(lambda z: get_distinct_count(z),IntegerType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_df.select(\"product_photos_qty\").distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[summary: string]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "describe_features_df.select(\"summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid argument, not a string or column: DataFrame[product_id: string, product_category_name: string, product_name_lenght: string, product_description_lenght: string, product_photos_qty: string, product_weight_g: string, product_length_cm: string, product_height_cm: string, product_width_cm: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-93-ec6ff5785f65>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdescribe_features_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwithColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mget_distinct_count_udf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"summary\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#.show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# df.select(col(\"Seqno\"), \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#     convertUDF(col(\"Name\")).alias(\"Name\") ) \\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#    .show(truncate=False)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    197\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massigned\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0massignments\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/udf.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *cols)\u001b[0m\n\u001b[1;32m    177\u001b[0m         \u001b[0mjudf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_judf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjudf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_to_seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_to_java_column\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m     \u001b[0;31m# This function is for improving the online help system in the interactive interpreter.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_seq\u001b[0;34m(sc, cols, converter)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \"\"\"\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconverter\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m         \u001b[0mcols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mconverter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcols\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/fastai_venv/lib/python3.8/site-packages/pyspark/sql/column.py\u001b[0m in \u001b[0;36m_to_java_column\u001b[0;34m(col)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mjcol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_column_from_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         raise TypeError(\n\u001b[0m\u001b[1;32m     46\u001b[0m             \u001b[0;34m\"Invalid argument, not a string or column: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0;34m\"{0} of type {1}. \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid argument, not a string or column: DataFrame[product_id: string, product_category_name: string, product_name_lenght: string, product_description_lenght: string, product_photos_qty: string, product_weight_g: string, product_length_cm: string, product_height_cm: string, product_width_cm: string] of type <class 'pyspark.sql.dataframe.DataFrame'>. For column literals, use 'lit', 'array', 'struct' or 'create_map' function."
     ]
    }
   ],
   "source": [
    "describe_features_df.withColumn(\"type\", get_distinct_count_udf(col(\"summary\"), features_df))#.show(truncate=False)\n",
    "\n",
    "# df.select(col(\"Seqno\"), \\\n",
    "#     convertUDF(col(\"Name\")).alias(\"Name\") ) \\\n",
    "#    .show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import *\n",
    "# from pyspark.sql import SparkSession\n",
    "\n",
    "# def main():\n",
    "#     DataList = [(\"Shirts\", 10, 13, 34, 10), (\"Trousers\", 11, 2, 30, 20), (\"Pants\", 70, 43, 24, 60), (\"Sweater\", 101, 44, 54, 80)]\n",
    "#     productQtyDF  = spark.createDataFrame(DataList, [\"Products\", \"Small\", \"Medium\", \"Large\", \"ExLarge\"])\n",
    "#     productQtyDF.show()\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "#     #|Products|Small|Medium|Large|ExLarge|\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "#     #|  Shirts|   10|    13|   34|     10|\n",
    "#     #|Trousers|   11|     2|   30|     20|\n",
    "#     #|   Pants|   70|    43|   24|     60|\n",
    "#     #| Sweater|  101|    44|   54|     80|\n",
    "#     #+--------+-----+------+-----+-------+\n",
    "\n",
    "#     productTypeDF = TransposeDF(productQtyDF, [\"Small\", \"Medium\", \"Large\", \"ExLarge\"], \"Products\")\n",
    "#     productTypeDF.show(truncate= False)\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "#     #|Products|Pants|Shirts|Sweater|Trousers|\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "#     #|Medium  |43   |13    |44     |2       |\n",
    "#     #|Small   |70   |10    |101    |11      |\n",
    "#     #|ExLarge |60   |10    |80     |20      |\n",
    "#     #|Large   |24   |34    |54     |30      |\n",
    "#     #+--------+-----+------+-------+--------+\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32952"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[_c0: string, _c1: string, _c2: string, _c3: string, _c4: string, _c5: string, _c6: string, _c7: string, _c8: string]>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.printSchema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|       _c0|                 _c1|                _c2|                 _c3|               _c4|             _c5|              _c6|              _c7|             _c8|\n",
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "|product_id|product_category_...|product_name_lenght|product_descripti...|product_photos_qty|product_weight_g|product_length_cm|product_height_cm|product_width_cm|\n",
      "+----------+--------------------+-------------------+--------------------+------------------+----------------+-----------------+-----------------+----------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
